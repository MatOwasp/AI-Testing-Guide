

# Threat Modeling for AI Systems

## What is Threat Modeling?

Threat modeling is a structured process for identifying, quantifying, and addressing security threats to a system. It allows developers, architects, and security professionals to proactively assess how their system could be attacked and to design appropriate defenses early in the development lifecycle.

In the context of AI systems, threat modeling helps uncover novel and evolving risks—ranging from prompt injection to model extraction—that stem from the unique properties of machine learning and generative AI technologies.

---

## Objectives

- Understand the **attack surface** of AI/ML systems
- Apply systematic methodologies to identify the threats. The following methodologies are applied to perfom the threat modeling on the AI System Architecture.

*Threat Modeling: Designing for Security*  
**Author:** Adam Shostack  
**URL:** [https://www.shostack.org/ThreatModeling](https://www.shostack.org/ThreatModeling)  
**Publisher:** Wiley, 2014  
**ISBN:** 978-1118809990  

> One of the most widely referenced books on structured threat modeling methodologies, especially in software and system design.

---

*Risk Centric Threat Modeling: Process for Attack Simulation and Threat Analysis*  
**Authors:** Tony UcedaVélez and Marco M. Morana  
**URL:** [https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling%3A+Process+for+Attack+Simulation+and+Threat+Analysis-p-9781118810040](https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling%3A+Process+for+Attack+Simulation+and+Threat+Analysis-p-9781118810040)  
**Publisher:** Wiley, 2015  
**ISBN:** 978-1118810040  

> This book introduces the **PASTA** (Process for Attack Simulation and Threat Analysis) methodology, which is risk-driven and highly applicable to modern AI systems and application environments.

## AI System Architecture

We base our analysis on the Google Secure AI Framework (SAIF), a public model for securing AI systems at scale.

🔗 [Google Secure AI Framework (SAIF) Architecture Map](https://saif.google/secure-ai-framework/saif-map)




<p align="center">
  <img src="/Document/images/AISystemArchitecture.png" alt="Description" width="1200"/>
</p>



## AI System Architecture Threat Modeling

This threat model analyzes the provided AI System Architecture. We've structured our approach around the Adam Shostack threat modeling methodology, using his approach as basis, contestualizing for our scope:

1. **What are we building?** (Architectural Decomposition)
2. **What can go wrong?** (Threat Identification)
3. **What are we doing about it?** (Mitigations)
4. **Did we do a good job?** (Validation and Review)

---

## Architectural Decomposition

Based on the provided AI system architecture, we identify these key components:



<p align="center">
  <img src="/Document/images/AI-System-Architecture-TM.png" alt="Description" width="800"/>
</p>



### **Application Layer**
(1) User input
(2) User output
(3) User Interface and interaction (Application)
(4) Agents/Plugins (Autonomous decision-making entities)
(5) External sources for Agents interactions

### **Model Layer**
(6) Input Handling
(7) Output Handling
(8) Model Usage (inference)

### **Infrastructure Layer**
(9) Model Storage Infrastructure
(10) Model Serving Infrastructure
(11) Model Evaluation
(12) Model Training & Tuning
(13) Model Frameworks and Code
(14) Data Storage Infrastructure

### **Data Layer**
(15) Training Data
(16) Data Filtering and Processing
(17) Data Sources
(18) External Data Sources (third-party providers)


---

# Comprehensive AI System Threat Model

The AI system architecture provided has been decomposed into components and communication channels (trusted boundaries). The threats to each are carefully identified according to the CIA (Confidentiality, Integrity, Availability) triad.

## Threat Analysis (CIA)
We did analyze each component of the architecture from a CIA point of view.

- **Confidentiality:**
  - MITM attacks on all communications
  - Unauthorized data interception across input/output, external sources
  - Exposure of sensitive or proprietary model communications

- **Integrity:**
  - Data injection, modification, and replay attacks on communication channels
  - Spoofing attacks against APIs and plugin communications
  - Injection of adversarial inputs or malicious payloads through communication channels

- **Availability:**
  - Resource exhaustion attacks on endpoints, infrastructure overload via communication flooding
  - Downtime and disruption caused by external dependency failures
  - Latency attacks, slow-loris style DoS on model serving endpoints
 
  The following are the identified threats.

---

## Identified CIA Threats 

### 🟦 **Application Layer Threats**

**(1) User Input Threats**
- Confidentiality: Sensitive data disclosure (PII leakage)
- Integrity: Prompt injection (direct or indirect), malicious payload injection
- Availability: DoS via high-complexity prompts, resource exhaustion

**(2) User Output Threats**
- Confidentiality: Unauthorized exposure of sensitive internal data
- Integrity: Output manipulation (misinformation, harmful biases, hallucinations)
- Availability: Output flooding, rate limiting circumvention

**(3) User Interface and Interaction Threats**
- Confidentiality: Credential or token leakage, session hijacking
- Integrity: UI spoofing, session fixation, parameter tampering
- Availability: UI DoS, frontend resource overload (excessive data queries)

**(4) Agents/Plugins (Autonomous Decision-Making Entities) Threats**
- Confidentiality: Privilege escalation, leakage of secret API keys or internal commands
- Integrity: Plugin manipulation, malicious instruction injection
- Availability: Resource exhaustion through automated requests, repeated agent calls (automation loops)

**(5) External Sources for Agent Interactions Threats (Communications)**
- Confidentiality: Man-in-the-middle (MITM) attacks, sensitive data interception
- Integrity: Data tampering during transmission, replay attacks
- Availability: External source downtime, communication latency, API rate-limiting attacks

---

### 🟪 **Model Layer Threats**

**(6) Input Handling Threats**
- Confidentiality: Leaking sensitive data through mishandling inputs
- Integrity: Input validation bypass, adversarial evasion attacks
- Availability: Input-based resource exhaustion (e.g., oversized inputs)

**(7) Output Handling Threats**
- Confidentiality: Leakage of sensitive inference results or hidden instructions
- Integrity: Generation of unsafe, biased, or manipulated output
- Availability: Performance degradation, excessive latency on output generation

**(8) Model Usage (Inference) Threats (Communications)**
- Confidentiality: Eavesdropping on inference requests/responses
- Integrity: MITM attacks altering inference results
- Availability: Inference denial via flooding, slow-loris attacks on inference endpoints

---

### 🟩 **Infrastructure Layer Threats**

**(9) Model Storage Infrastructure Threats**
- Confidentiality: Unauthorized access, model theft at rest
- Integrity: Unauthorized model modification, malicious tampering
- Availability: Data loss, hardware failures, ransomware attacks

**(10) Model Serving Infrastructure Threats**
- Confidentiality: Interception of inference results, compromised endpoints
- Integrity: Injection of malicious inference code, supply chain compromise
- Availability: DoS attacks, server overload, infrastructure sabotage

**(11) Model Evaluation Threats**
- Confidentiality: Exposure of sensitive evaluation data or techniques
- Integrity: Falsified or manipulated evaluation results
- Availability: Sabotage evaluation pipelines, resource starvation

**(12) Model Training & Tuning Threats**
- Confidentiality: Training data leakage, exposure of intellectual property (model weights)
- Integrity: Model poisoning (malicious training data or parameters), backdoor injections
- Availability: Resource-intensive attacks slowing or halting training

**(13) Model Frameworks and Code Threats**
- Confidentiality: Vulnerable code exploitation leading to unauthorized access
- Integrity: Code injection, dependency confusion attacks, supply chain compromise
- Availability: Exploits causing crashes, performance degradation, or downtime

**(14) Data Storage Infrastructure Threats**
- Confidentiality: Unauthorized data access, breaches, data exfiltration
- Integrity: Unauthorized modification or deletion of stored data
- Availability: Storage denial (disk flooding), hardware failures, ransomware encryption attacks

---

### 🟨 **Data Layer Threats**

**(15) Training Data Threats**
- Confidentiality: Data breaches, unauthorized access to sensitive datasets
- Integrity: Training data poisoning, intentional data corruption
- Availability: Data destruction, resource-heavy training data injections

**(16) Data Filtering and Processing Threats**
- Confidentiality: Sensitive data disclosure during processing
- Integrity: Filtering bypass, insertion of malicious processed data
- Availability: Resource-intensive filters or processors causing denial-of-service

**(17) Data Sources Threats (Communications)**
- Confidentiality: MITM interception during data transfer
- Integrity: Data tampering in transit, spoofed data sources
- Availability: Data source downtime, throttling, or denial-of-service

**(18) External Data Sources (Third-party Providers) Threats (Communications)**
- Confidentiality: Exposure of sensitive data via third-party breaches
- Integrity: Malicious manipulation of external datasets before ingestion
- Availability: External downtime, denial-of-service, rate-limiting attacks on external APIs

---

### Identify Threats

1. **Prompt Injection**
   - Direct: Users directly manipulating inputs.
   - Indirect: Inputs from external sources (e.g., files or databases) being maliciously altered to manipulate model behavior.

2. **Sensitive Information Disclosure**
   - Unauthorized inference of confidential data (training data, model weights, API details).

3. **Data and Model Poisoning**
   - Malicious training data introduction or manipulation during retraining.
   - Manipulation of model parameters or embedding layers.

4. **Improper Output Handling**
   - Inaccurate, biased, or harmful outputs due to insufficient validation of model outputs.

5. **Excessive Agency**
   - AI systems operating beyond their intended scope, potentially causing unintended harmful actions.

6. **System Prompt Leakage**
   - Exposing confidential instructions provided in prompts, which should remain private.

7. **Vector and Embedding Weaknesses**
   - Attacks on retrieval-augmented generation (RAG) systems or embedding layers leading to model manipulation.

- **Integrity Violations**
  - Model poisoning, targeted attacks, evasion attacks through adversarial inputs.

- **Availability Violations**
  - Denial of service (DoS) via model poisoning or latency-energy attacks.

- **Privacy Compromise**
  - Extraction of private information through membership inference, model extraction, or training data reconstruction.

- **Supply Chain Attacks**
  - Compromise of external libraries or components used within the AI infrastructure.

---

### Step 4: Potential Attack Vectors in the AI Architecture:

**User/API layer threats:**
- Direct prompt injection via API calls.
- API abuse to access unauthorized functionality.

**Model threats:**
- Poisoned data inputs causing integrity violations.
- Manipulation through indirect prompt injection or compromised embeddings.

**Infrastructure threats:**
- API exploitation for resource exhaustion (unbounded consumption).
- Compromise of cloud infrastructure hosting models or data storage.



## References

- **OWASP Top 10 for LLM Applications**  
  [https://genai.owasp.org/llmrisk/](https://genai.owasp.org/llmrisk/)
  
- **OWASP AI Exchange Threats Through Use**  
  [https://owaspai.org/docs/2_threats_through_use/](https://owaspai.org/docs/2_threats_through_use/)

- **"Threat Modeling: Designing for Security"**  
  Adam Shostack  
  [https://www.shostack.org/ThreatModeling](https://www.shostack.org/ThreatModeling)

- **“Risk-Centric Threat Modeling” (PASTA Methodology)**  
  Tony UcedaVélez, Marco Morana  
  [https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling](https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling%3A+Process+for+Attack+Simulation+and+Threat+Analysis-p-9781118810040)





---
NEXT:
2.1 [Identify AI Threats](2.1IdentifyAIThreats.md)

[Table of Contents](/Document/README.md)
