

# Threat Modeling for AI Systems

## What is Threat Modeling?

Threat modeling is a structured process for identifying, quantifying, and addressing security threats to a system. It allows developers, architects, and security professionals to proactively assess how their system could be attacked and to design appropriate defenses early in the development lifecycle.

In the context of AI systems, threat modeling helps uncover novel and evolving risksâ€”ranging from prompt injection to model extractionâ€”that stem from the unique properties of machine learning and generative AI technologies.

---

## Objectives

- Understand the **attack surface** of AI/ML systems
- Apply systematic methodologies to identify the threats. The following methodologies are applied to perfom the threat modeling on the AI System Architecture.

*Threat Modeling: Designing for Security*  
**Author:** Adam Shostack  
**URL:** [https://www.shostack.org/ThreatModeling](https://www.shostack.org/ThreatModeling)  
**Publisher:** Wiley, 2014  
**ISBN:** 978-1118809990  

> One of the most widely referenced books on structured threat modeling methodologies, especially in software and system design.

---

*Risk Centric Threat Modeling: Process for Attack Simulation and Threat Analysis*  
**Authors:** Tony UcedaVÃ©lez and Marco M. Morana  
**URL:** [https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling%3A+Process+for+Attack+Simulation+and+Threat+Analysis-p-9781118810040](https://www.wiley.com/en-us/Risk+Centric+Threat+Modeling%3A+Process+for+Attack+Simulation+and+Threat+Analysis-p-9781118810040)  
**Publisher:** Wiley, 2015  
**ISBN:** 978-1118810040  

> This book introduces the **PASTA** (Process for Attack Simulation and Threat Analysis) methodology, which is risk-driven and highly applicable to modern AI systems and application environments.

## AI System Architecture

We base our analysis on the Google Secure AI Framework (SAIF), a public model for securing AI systems at scale.

ðŸ”— [Google Secure AI Framework (SAIF) Architecture Map](https://saif.google/secure-ai-framework/saif-map)


<p align="center">
  <img src="AISystemArchitecture-png" alt="Description" width="600"/>
</p>



### Step 1: Identify Assets

From the above diagram, the assets likely include:

- **Data Inputs**
  - Raw input data
  - User data
  - External data sources
  
- **Models**
  - Predictive and generative models
  - Embedded multimodal components
  
- **Infrastructure**
  - Cloud-based computing infrastructure
  - APIs and model endpoints
  
- **Outputs**
  - Generated results
  - Decisions or recommendations provided by the AI

---

### Step 2: Define Trust Boundaries

- User â†” API layer
- API layer â†” AI Models
- AI Models â†” Database/storage systems
- AI Models â†” External systems

---

### Step 3: Identify Threats

1. **Prompt Injection**
   - Direct: Users directly manipulating inputs.
   - Indirect: Inputs from external sources (e.g., files or databases) being maliciously altered to manipulate model behavior.

2. **Sensitive Information Disclosure**
   - Unauthorized inference of confidential data (training data, model weights, API details).

3. **Data and Model Poisoning**
   - Malicious training data introduction or manipulation during retraining.
   - Manipulation of model parameters or embedding layers.

4. **Improper Output Handling**
   - Inaccurate, biased, or harmful outputs due to insufficient validation of model outputs.

5. **Excessive Agency**
   - AI systems operating beyond their intended scope, potentially causing unintended harmful actions.

6. **System Prompt Leakage**
   - Exposing confidential instructions provided in prompts, which should remain private.

7. **Vector and Embedding Weaknesses**
   - Attacks on retrieval-augmented generation (RAG) systems or embedding layers leading to model manipulation.

- **Integrity Violations**
  - Model poisoning, targeted attacks, evasion attacks through adversarial inputs.

- **Availability Violations**
  - Denial of service (DoS) via model poisoning or latency-energy attacks.

- **Privacy Compromise**
  - Extraction of private information through membership inference, model extraction, or training data reconstruction.

- **Supply Chain Attacks**
  - Compromise of external libraries or components used within the AI infrastructure.

---

### Step 4: Potential Attack Vectors in the AI Architecture:

**User/API layer threats:**
- Direct prompt injection via API calls.
- API abuse to access unauthorized functionality.

**Model threats:**
- Poisoned data inputs causing integrity violations.
- Manipulation through indirect prompt injection or compromised embeddings.

**Infrastructure threats:**
- API exploitation for resource exhaustion (unbounded consumption).
- Compromise of cloud infrastructure hosting models or data storage.


