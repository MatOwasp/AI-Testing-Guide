# 3. AI Testing Guide Framework

Based on the Threat modeling performed at Chapter 2, we can now define a structured framework that maps the most critical security and trustworthy AI threats to concrete test cases. This project aims to bridge traditional cybersecurity, MLOps testing, and Responsible AI assessments under a unified structure.

Each test case is categorized under one of the four pillars, based on the threat modeling we did perform in the previous chapters:

- ðŸŸ¦ **AI Application Testing**
- ðŸŸª **AI Model Testing**
- ðŸŸ© **AI Infrastructure Testing**
- ðŸŸ¨ **AI Data Testing**

---
In our framework we use prompt injections and prompt attacks to indenfity the vulnerabilities. We define the following three kind of Input for an AI System:

<p align="center">
  <img src="/Document/images/Prompt.png" alt="Description" width="800"/>
</p>

**1) Prompt (Intended Prompt)**  
This is the standard, intended input given to the AI system by the designer or user without any malicious intent or attempt to manipulate the system. This type of input aligns with the intended functionality and expected behavior of the AI model.

- **Example:** Asking the AI to summarize an article, answer a customer query, or generate text according to specified instructions.


**2) Prompt Injection**  
This type of input refers to scenarios where the user intentionally or unintentionally provides prompts designed to manipulate or override the intended behavior of the AI system, causing it to produce unintended, possibly harmful, or sensitive outputs. It can be divided into two categories:

- **Direct Prompt Injection:** User directly inputs text commands explicitly intended to manipulate model behavior.
- **Indirect Prompt Injection:** Malicious prompts embedded within externally sourced content (e.g., web pages, documents) which the model ingests unknowingly.

- **Example:** Injecting instructions to bypass security constraints, reveal private system information, or disregard programmed ethical guidelines.


**3) Prompt Attack**  
Prompt attacks involve specific manipulative prompts designed to exploit inherent model vulnerabilities related to hallucinations (model inventing or confidently providing incorrect information) and Responsible AI (RAI) and Trustworthy considerations (ethical guidelines, fairness, transparency). This category explicitly deals with exploiting known weaknesses in the AI modelâ€™s accuracy or ethical controls.

- **Hallucination Attack:** Crafting prompts designed to exploit the model's propensity for fabricating believable but incorrect responses.
  - **Example:** Asking the AI for non-existent historical facts or scientific data, leading to plausible yet entirely fabricated responses.

- **Responsible AI Attack (RAI Attack):** Prompts intentionally exploiting or subverting ethical and fairness-related constraints.
  - **Example:** Manipulating the model to generate biased, toxic, or inappropriate outputs, contrary to established ethical standards.

---

The following are the list of tests to perform for each of the 4 cathegories:

### ðŸŸ¦ AI Application Testing

| Test ID       | Test Name | Threat Source |
|---------------|-----------|----------------|
| AITG-APP-01   | [Testing for Prompt Injection](/Document/content/tests/AITG-APP-01_Testing_for_Prompt_Injection.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-02   | [Testing for Indirect Prompt Injection](/Document/content/tests/AITG-APP-02_Testing_for_Indirect_Prompt_Injection.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-03   | [Testing for Sensitive Data Leak](/Document/content/tests/AITG-APP-03_Testing_for_Sensitive_Data_Leak.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-04   | [Testing for Input Leakage](/Document/content/tests/AITG-APP-04_Testing_for_Input_Leakage.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-05   | [Testing for Unsafe Outputs](/Document/content/tests/AITG-APP-05_Testing_for_Unsafe_Outputs.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-06   | [Testing for Agentic Behavior Limits](/Document/content/tests/AITG-APP-06_Testing_for_Agentic_Behavior_Limits.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-07   | [Testing for Prompt Disclosure](/Document/content/tests/AITG-APP-07_Testing_for_Prompt_Disclosure.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-08   | [Testing for Embedding Manipulation](/Document/content/tests/AITG-APP-08_Testing_for_Embedding_Manipulation.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-09   | [Testing for Model Extraction](/Document/content/tests/AITG-APP-09_Testing_for_Model_Extraction.md) | OWASP AI Exchange |
| AITG-APP-10   | [Testing for Harmful Content Bias](/Document/content/tests/AITG-APP-10_Testing_for_Harmful_Content_Bias.md) | OWASP Top 10 LLM 2025 |
| AITG-APP-11   | [Testing for Hallucinations](/Document/content/tests/AITG-APP-11_Testing_for_Hallucinations.md) | Trustworthy AI |
| AITG-APP-12   | [Testing for Toxic Output](/Document/content/tests/AITG-APP-12_Testing_for_Toxic_Output.md) | Responsible AI |
| AITG-APP-13   | [Testing for Over-Reliance on AI](/Document/content/tests/AITG-APP-13_Testing_for_Over-Reliance_on_AI.md) | Responsible AI |
| AITG-APP-14   | [Testing for Explainability and Interpretability](/Document/content/tests/AITG-APP-14_Testing_for_Explainability_and_Interpretability.md) | Responsible AI |


### ðŸŸª AI Model Testing

| Test ID       | Test Name | Threat Source |
|---------------|-----------|----------------|
| AITG-MOD-01   | [Testing for Evasion Attacks](/Document/content/tests/AITG-MOD-01_Testing_for_Evasion_Attacks.md) | OWASP AI Exchange |
| AITG-MOD-02   | [Testing for Runtime Model Poisoning](/Document/content/tests/AITG-MOD-02_Testing_for_Runtime_Model_Poisoning.md) | OWASP Top 10 LLM 2025 |
| AITG-MOD-03   | [Testing for Poisoned Training Sets](/Document/content/tests/AITG-MOD-03_Testing_for_Poisoned_Training_Sets.md) | OWASP Top 10 LLM 2025 |
| AITG-MOD-04   | [Testing for Fine-tuning Poisoning](/Document/content/tests/AITG-MOD-04_Testing_for_Fine-tuning_Poisoning.md) | OWASP Top 10 LLM 2025 |
| AITG-MOD-05   | [Testing for Membership Inference](/Document/content/tests/AITG-MOD-05_Testing_for_Membership_Inference.md) | OWASP AI Exchange |
| AITG-MOD-06   | [Testing for Inversion Attacks](/Document/content/tests/AITG-MOD-06_Testing_for_Inversion_Attacks.md) | OWASP AI Exchange |
| AITG-MOD-07   | [Testing for Robustness to New Data](/Document/content/tests/AITG-MOD-07_Testing_for_Robustness_to_New_Data.md) | Trustworthy AI |
| AITG-MOD-08   | [Testing for Goal Alignment](/Document/content/tests/AITG-MOD-08_Testing_for_Goal_Alignment.md) | Trustworthy AI |

---

### ðŸŸ© AI Infrastructure Testing

| Test ID       | Test Name | Threat Source |
|---------------|-----------|----------------|
| AITG-INF-01   | [Testing for Supply Chain Tampering](/Document/content/tests/AITG-INF-01_Testing_for_Supply_Chain_Tampering.md) | OWASP Top 10 LLM 2025 |
| AITG-INF-02   | [Testing for Resource Exhaustion](/Document/content/tests/AITG-INF-02_Testing_for_Resource_Exhaustion.md) | OWASP Top 10 LLM 2025 |
| AITG-INF-03   | [Testing for Plugin Boundary Violations](/Document/content/tests/AITG-INF-03_Testing_for_Plugin_Boundary_Violations.md) | Trustworthy AI |
| AITG-INF-04   | [Testing for Capability Misuse](/Document/content/tests/AITG-INF-04_Testing_for_Capability_Misuse.md) | Responsible AI |
| AITG-INF-05   | [Testing for Fine-tuning Poisoning](/Document/content/tests/AITG-INF-05_Testing_for_Fine-tuning_Poisoning.md) | OWASP Top 10 LLM 2025 |
| AITG-INF-06   | [Testing for Dev-Time Model Theft](/Document/content/tests/AITG-INF-06_Testing_for_Dev-Time_Model_Theft.md) | OWASP AI Exchange |

---

### ðŸŸ¨ AI Data Testing

| Test ID       | Test Name | Threat Source |
|---------------|-----------|----------------|
| AITG-DAT-01   | [Testing for Training Data Exposure](/Document/content/tests/AITG-DAT-01_Testing_for_Training_Data_Exposure.md) | OWASP AI Exchange |
| AITG-DAT-02   | [Testing for Runtime Exfiltration](/Document/content/tests/AITG-DAT-02_Testing_for_Runtime_Exfiltration.md) | OWASP AI Exchange |
| AITG-DAT-03   | [Testing for Dataset Diversity & Coverage](/Document/content/tests/AITG-DAT-03_Testing_for_Dataset_Diversity_and_Coverage.md) | Responsible AI |
| AITG-DAT-04   | [Testing for Harmful Content in Data](/Document/content/tests/AITG-DAT-04_Testing_for_Harmful_Content_in_Data.md) | Responsible AI |
| AITG-DAT-05   | [Testing for Data Minimization & Consent](/Document/content/tests/AITG-DAT-05_Testing_for_Data_Minimization_and_Consent.md) | Trustworthy AI |


NEXT:
[3.1 Testing for Prompt Injection](https://github.com/MatOwasp/AI-Testing-Guide/blob/main/Document/content/Document/content/tests/AITG-APP-01_Testing_for_Prompt_Injection.md)     
[Table of Content](README.md)
